{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gjaischool\\Documents\\GitHub\\D.O2\\Ai Model\\김동욱\\231108 RNN.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gjaischool/Documents/GitHub/D.O2/Ai%20Model/%EA%B9%80%EB%8F%99%EC%9A%B1/231108%20RNN.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gjaischool/Documents/GitHub/D.O2/Ai%20Model/%EA%B9%80%EB%8F%99%EC%9A%B1/231108%20RNN.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gjaischool/Documents/GitHub/D.O2/Ai%20Model/%EA%B9%80%EB%8F%99%EC%9A%B1/231108%20RNN.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpprint\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>후기 남길게요 오븐 스파게티 서비스로 주세요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>혹시 코스요리에서 파스타만 포장 가능할까요?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sentence  label_idx\n",
       "0  후기 남길게요 오븐 스파게티 서비스로 주세요          0\n",
       "1  혹시 코스요리에서 파스타만 포장 가능할까요?          0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(15726, 2)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"data.xlsx\", engine=\"openpyxl\")#.iloc[:100]\n",
    "# data[\"label_idx\"] = [random.choice([0,1,2,3,4]) for _ in range(100)]\n",
    "display(data.head(2))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(all_indices))\n",
    "val_size = len(all_indices) - train_size\n",
    "train_idx, val_idx = all_indices[:int(0.8 * len(all_indices))], all_indices[int(0.8 * len(all_indices)):]\n",
    "\n",
    "num_classes = max(data[\"label_idx\"].tolist())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RangeIndex(start=0, stop=80, step=1), RangeIndex(start=80, stop=100, step=1))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# transformers 라이브러리에서 BERT 토크나이저를 임포트합니다.\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 다국어 지원 BERT 모델을 사용하여 토크나이저 객체를 생성합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.encodings = [\n",
    "            tokenizer.encode_plus(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512) for sentence in df[\"sentence\"].tolist()\n",
    "        ]\n",
    "        self.labels = df[\"label_idx\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # to_return = {\n",
    "        #     \"sentence_encoded\":self.encodings[idx],\n",
    "        #     \"label\":self.labels[idx]\n",
    "        # }\n",
    "        return (self.encodings[idx], self.labels[idx])\n",
    "        # return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.iloc[train_idx]\n",
    "val_data = data.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(train_data, tokenizer)\n",
    "val_dataset = TextClassificationDataset(val_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[    0,  7541, 20206,  7187, 18301, 23115,  3838,  2200, 27616,  2182,\n",
       "              2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " 3)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/dwook.kim/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"klue/roberta-small\")\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes, hidden_size):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.model = bert_model\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes) \n",
    "    \n",
    "    def forward(self, _input):\n",
    "        output = self.model(**_input)[\"pooler_output\"]\n",
    "        output = self.fc(output[0])\n",
    "        return output\n",
    "\n",
    "custom_model = CustomModel(model, num_classes, 768)\n",
    "optimizer = AdamW(custom_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = loss_f(F.softmax(outputs), _label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.4785, -3.5743, 11.4758, -2.6121, -2.7977], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  all_predictions.append( torch.argmax(F.softmax(outputs)).item() )\n",
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  all_labels.append( torch.argmax(F.softmax(_label)).item() )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.01818181818181818\n",
      "Epoch 2/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = loss_f(F.softmax(outputs), _label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.5633, -3.6032, 11.5367, -2.5237, -2.8219], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  all_predictions.append( torch.argmax(F.softmax(outputs)).item() )\n",
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  all_labels.append( torch.argmax(F.softmax(_label)).item() )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.01818181818181818\n",
      "Epoch 3/3\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = loss_f(F.softmax(outputs), _label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.3328, -3.3854, 10.9299, -2.3194, -2.6250], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  all_predictions.append( torch.argmax(F.softmax(outputs)).item() )\n",
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/4052897966.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  all_labels.append( torch.argmax(F.softmax(_label)).item() )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.01818181818181818\n"
     ]
    }
   ],
   "source": [
    "loss_f = nn.CrossEntropyLoss()\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print('-' * 10)\n",
    "    # 모델을 학습 모드로 설정합니다.\n",
    "    model.train()\n",
    "    for i, (_input, _label) in enumerate(train_loader):\n",
    "        _input = {k:v.squeeze(0) for k, v in _input.items()}\n",
    "        _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
    "        \n",
    "        outputs = custom_model(_input)\n",
    "        loss = loss_f(F.softmax(outputs), _label)\n",
    "        loss.backward() # 역전파를 수행합니다.\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(outputs)\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, (_input, _label) in enumerate(val_loader):\n",
    "        _input = {k:v.squeeze(0) for k, v in _input.items()}\n",
    "        _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
    "        \n",
    "        outputs = custom_model(_input)\n",
    "        #loss = loss_f(F.softmax(outputs), _label)\n",
    "        all_predictions.append( torch.argmax(F.softmax(outputs)).item() )\n",
    "        all_labels.append( torch.argmax(F.softmax(_label)).item() )\n",
    "        \n",
    "\n",
    "\n",
    "    print(accuracy_score(all_labels, all_predictions))\n",
    "    print(f1_score(all_labels, all_predictions, average=\"weighted\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/3637145079.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/3637145079.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  all_predictions.append( torch.argmax(F.softmax(outputs)).item() )\n",
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/3637145079.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  all_labels.append( torch.argmax(F.softmax(_label)).item() )\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1002,  0.0965,  0.0802, -0.0014, -0.3126], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습하기 이전 inference 결과\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.8254, -3.8895, 12.4735, -2.9632, -3.1166], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습하기 이전 inference 결과\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/3142704713.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(outputs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([8.3459e-08, 7.8275e-08, 1.0000e+00, 1.9766e-07, 1.6954e-07],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/3085399197.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.argmax(F.softmax(outputs)).item()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(F.softmax(outputs)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.6564, -3.7143, 11.9799, -2.7231, -2.9375], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
