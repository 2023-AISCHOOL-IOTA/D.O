{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c15c536-00da-4303-a255-b577df39a35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/Project/SJ'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 나의 작업 위치 확인\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ca91126-7ca4-4a64-a8be-549954de014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # pandas 라이브러리 pd로 불러옴\n",
    "data = pd.read_excel('/home/ubuntu/Project/SJ/Data9.xlsx') # Data9 엑셀 파일을 불러서 data에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23d41619-45ca-4a78-b752-5f50a86d7725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>발화자</th>\n",
       "      <th>발화문</th>\n",
       "      <th>QA여부</th>\n",
       "      <th>인텐트</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c</td>\n",
       "      <td>참치타다끼샐러드는 취소했었는데, 맞죠?</td>\n",
       "      <td>q</td>\n",
       "      <td>주문_취소_확인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s</td>\n",
       "      <td>네, 참치타다끼샐러드는 취소된 상태입니다.</td>\n",
       "      <td>a</td>\n",
       "      <td>주문_취소_확인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>주문한 메뉴 취소할 때 주문서만 있으면 된다는 말씀이세요?</td>\n",
       "      <td>q</td>\n",
       "      <td>주문_취소_확인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s</td>\n",
       "      <td>네, 주문서만 있으면 취소 가능합니다.</td>\n",
       "      <td>a</td>\n",
       "      <td>주문_취소_확인</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c</td>\n",
       "      <td>고객서비스가 엉망이군요. 그냥 갑니다.</td>\n",
       "      <td>q</td>\n",
       "      <td>주문_취소_확인</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  발화자                               발화문 QA여부       인텐트\n",
       "0   c             참치타다끼샐러드는 취소했었는데, 맞죠?    q  주문_취소_확인\n",
       "1   s           네, 참치타다끼샐러드는 취소된 상태입니다.    a  주문_취소_확인\n",
       "2   c  주문한 메뉴 취소할 때 주문서만 있으면 된다는 말씀이세요?    q  주문_취소_확인\n",
       "3   s             네, 주문서만 있으면 취소 가능합니다.    a  주문_취소_확인\n",
       "4   c             고객서비스가 엉망이군요. 그냥 갑니다.    q  주문_취소_확인"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96553ad-4619-4eaf-9ef4-a2b26b1586fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "import torch # PyTorch 라이브러리 가져오기 / PyTorch : 딥러닝 모델 구축시켜 학습시키는데 사용되는 라이브러리\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast # Hugging face의 transformer라이브러리에서 GPT2LMHeadModel, PreTrainedTokenizerFast 모델을 불러옴\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2') # 사전에 학습된 skt kogpt2를 model에 불러옴\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>',\n",
    "    unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') # 사전에 학습된 skt kogpt2를 토크나이저 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142846ef-d5ce-4d38-af35-7d0f35de25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids : 토큰화된 텍스트를 나타내는 숫자 ID 시퀀스\n",
    "# ex) '안녕하세요'  -> '안녕','하세요' -> 17,123 이런식으로 변한 숫자의 시퀀스가 input_ids\n",
    "# 그럼 시퀀스는 뭐냐 -> 17, 123으로 변한 이런 숫자를 연속적으로 나열한거를 시퀀스라 한다\n",
    "# 즉 순서대로 나열된 요소들의 연속체이다.\n",
    "\n",
    "# attention_mask : 모델이 주의를 기울여야 할 토큰\n",
    "# transformer는 모든 입력이 일정한 길이를 가져야한다 그래서 max_length를 정해놓고 남은 부분은 padding을 통해서 채우는데,\n",
    "# 이렇게 패딩된 부분을 모델이 무시하게 해주는 역할\n",
    "# ex) max_length = 5 , padding = max_length 일때 '안녕하세요' -> '안녕','하세요' -> 17, 123 -> [1, 1, 0, 0, 0]\n",
    "# 여기서 1이 바로 주의를 기울여야할 토큰, 0은 무시하는 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "159f6baf-17c8-4f3e-aebd-8966e510a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset # Dataset 로드\n",
    "class ChatDataset(Dataset): # torch.utils.data.Dataset을 상속받는 ChatDataset클래스 생성\n",
    "    def __init__(self, conversations, tokenizer, max_length=512): # 클래스 초기화 생성자, 대화데이터 conversations와 tokenizer, 최대길이를 입력받아 클래스 초기화\n",
    "        self.tokenizer = tokenizer #전달받은 토크나이저 객체를 클래스의 인스턴스 변수로 저장\n",
    "        self.inputs = []           #토큰화된 텍스트의 입력 ID들을 저장하는 리스트\n",
    "        self.attention_masks = []  # attention_masks를 저장할 리스트\n",
    "\n",
    "        for conv in conversations: # 반복문 대화에 의해 실행 \n",
    "            # 각 대화 쌍의 질문과 답변을 결합하고, 각 부분 사이에 토크나이저의 끝경계 토큰을 삽입(더 잘 학습하기위해)\n",
    "            combined_text = f\"{conv['question']} {tokenizer.eos_token} {conv['answer']} {tokenizer.eos_token}\"\n",
    "            \n",
    "            # 결합된 텍스트 conbined_text를 토크나이저를 사용해 토큰화/ 길이는 max_length만큼, max_length만큼 채워주게 padding\n",
    "            tokenized_text = tokenizer(combined_text, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "            # 텍스트의 토큰화된 입력id들을 self.inputs리스트에 추가\n",
    "            self.inputs.append(torch.tensor(tokenized_text['input_ids']))\n",
    "            \n",
    "            # 텍스트의 토큰화된 attention_mask들을 self.attenstion_masks 리스트에 추가\n",
    "            self.attention_masks.append(torch.tensor(tokenized_text['attention_mask']))\n",
    "    def __len__(self): # ChatDataset의 길이를 반환\n",
    "        return len(self.inputs) # self.inputs 리스트의 길이를 반환 ( 데이터셋에 포함된 대화 쌍의 수)\n",
    "\n",
    "    def __getitem__(self, idx): # 특정 인덱스인 idx에 해당하는 데이터 쌍을 반환 (데이터 쌍 : input_ids + attention_masks)\n",
    "        return self.inputs[idx], self.attention_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49b5934d-5f25-4369-b944-b7706855a2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 변환\n",
    "def build_dataset(data, tokenizer):    # 위에서 정의한 data와 tokenizer를 매개변수로 받아 새로운 데이터셋을 구축하는 함수\n",
    "    conversations = []                 # conversations(대화) 라는 질문과 답변을 저장할 리스트 초기화\n",
    "    for i in range(len(data) - 1):     # data의 길이 -1 만큼 for문을 돌릴건데, 왜 -1을 해야하느냐? -> 쌍을 지어야하는데 마지막 까지 가버리면 마지막이 A일때 쌍을 지을 수 없으니까!\n",
    "        if data.iloc[i]['QA여부'] == 'q' and data.iloc[i+1]['QA여부'] == 'a': # for문을 돌면서 conversations에 질문과 답변을 쌍을 지어 추가\n",
    "            conversations.append({\n",
    "                'question': data.iloc[i]['발화문'],\n",
    "                'answer': data.iloc[i+1]['발화문']\n",
    "            })\n",
    "    return conversations \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5419995-4960-41e1-83cd-0e9aec2f37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = build_dataset(data, tokenizer) # 위에서 언급한 build_dataset함수를 사용해서 대화 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17dd88cd-48d0-469f-814f-ae152cddd7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 전체 데이터셋을 학습 데이터셋과 검증 데이터셋으로 9:1로 분리\n",
    "train_conversations, val_conversations = train_test_split(conversations, test_size=0.1)\n",
    "\n",
    "# 학습 및 검증 데이터셋을 위한 DataLoader 생성 \n",
    "train_dataset = ChatDataset(train_conversations, tokenizer) #위에서 언급했던 ChatDataset 클래스 활용해서 데이터셋 설정\n",
    "val_dataset = ChatDataset(val_conversations, tokenizer)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # 위에서 불러온 DataLoader를 이용\n",
    "validation_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # 그럼 DataLoader는 뭐냐?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba044b6-4f0e-429b-834b-82514a8ecec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader는 PyTorch에서 제공하는 유틸리티 클래스\n",
    "# 모델 학습을 위해 데이터를 효율적으로 로딩하고 배치처리를 수행하는 역할\n",
    "# 그럼 여기서 배치는 뭐냐? 한번에 처리할 수 있는 일종의 묶음이라고 보면됨.\n",
    "# 병렬로 묶어서 한번에 계산을 하는느낌인데 이때 너무 batch를 키우면 메모리가 터짐\n",
    "# 셔플링도 가능 -> shuffle=True이면 각에포크가 시작할때 랜덤으로 데이터셋을 섞음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "634b048f-8f31-4d47-8dd9-4030df2aaffe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# model.parameters의 파라미터는 매개변수를 의미, 즉 파라미터 = 매개변수 = 가중치, 편향\n",
    "# 즉 모델의 파라미터를 최적화 하기 위해서 lr을 10^-5 x 5로 옵티마이저를 설정하는 과정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5) \n",
    "\n",
    "# 학습 설정\n",
    "epochs = 5 # 에폭설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # cuda(GPU)쓸거다 없으면 cpu쓸거다는 뜻\n",
    "model.to(device) # 모델을 GPU로 돌릴거야!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df580600-53f2-4608-bf38-85ffb1677db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3358/3358 [21:55<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed. Average loss: 0.10599597483238803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3358/3358 [21:55<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 completed. Average loss: 0.0765620187100216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3358/3358 [21:56<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 completed. Average loss: 0.06841152783831611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3358/3358 [23:31<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 completed. Average loss: 0.06126628282628044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3358/3358 [47:19<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 completed. Average loss: 0.055123316653254775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs): # 위에서 설정한 에폭만큼 epoch돌릴거야!\n",
    "    model.train() #모델을 학습모드로 설정\n",
    "    total_loss = 0 # 전체 로스 초기화\n",
    "    for batch in tqdm(train_loader):   # 학습 데이터셋의 배치에 대해 반복\n",
    "        # 배치 데이터를 GPU로 옮김 (필요한 경우)\n",
    "        inputs, masks = [x.to(device) for x in batch] # inputs, masks 데이터 GPU로 이동시켜 실행 / PyTorch는 모두 같은 device에서 실행해야 오류가 안남\n",
    "\n",
    "        # 그레이디언트 초기화\n",
    "        optimizer.zero_grad() # 이전 그레디언트가 영향을 끼치지 않도록 초기화\n",
    "\n",
    "        # 모델의 결과를 얻음\n",
    "        outputs = model(input_ids=inputs, attention_mask=masks, labels=inputs) # 이부분은 아래에 설명\n",
    "        loss = outputs.loss # 출력의 손실을 loss에 저장\n",
    "\n",
    "        # 역전파 수행\n",
    "        loss.backward() # 손실에 대한 그레디언트 계산해서 가중치를 어떻게 조절할지 결정\n",
    "        optimizer.step() # 다시 옵티마이저를 통해 가중치를 조절하고\n",
    "\n",
    "        total_loss += loss.item() # 총 손실을 업데이트해서 보여줌\n",
    "    \n",
    "    # 에포크마다 평균 손실을 출력\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} completed. Average loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f5511e-b1e6-4e39-8de4-86f7dab9dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(input_ids=inputs, attention_mask=masks, labels=inputs)\n",
    "# model은 우리가 위에서 설정햇듯 sktkogpt2이고\n",
    "# input_ids역시 위에서 설명햇듯이 입력된 텍스트를 숫자로 저장한 시퀀스\n",
    "# inputs은 batch를 실행한 입력 데이터를 의미\n",
    "# attention_mask 역시 위에서 설명했듯 어디에 집중을 할지\n",
    "# mask는 batch의 attention_mask를 나타냄\n",
    "# labels = inputs은 labels에 inputs을 전달하는건데 일종의 자기주도학습으로 입력을 정확하게 재현하기 위해 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42d4d056-c3e4-421e-ae91-9f4cd0292669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token='</s>', eos_token='</s>',\n",
    "    unk_token='<unk>', pad_token='<pad>', mask_token='<mask>',\n",
    "    padding_side='left'\n",
    ") # 이건 위에서 한 것과 같은데 패딩만 오른쪽으로 하다가 왼쪽으로 하는 코드만 추가 / [1,1,0,0,0] -> [0,0,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3d08dad-a17e-41f1-847c-2772c0377006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "# BLEU 점수 계산을 위한 함수\n",
    "def calculate_bleu(reference_texts, generated_text):\n",
    "    # 참조 텍스트를 토큰화\n",
    "    reference_tokens = [word_tokenize(ref) for ref in reference_texts]\n",
    "    # 생성된 텍스트를 토큰화\n",
    "    generated_tokens = word_tokenize(generated_text)\n",
    "\n",
    "    smoothing = SmoothingFunction().method1 # 0으로 나눠버리는 등의 문제를 방지하기위해 Smoothing함수(method1) 사용\n",
    "    return sentence_bleu(reference_tokens, generated_tokens, smoothing_function=smoothing)\n",
    "    # sentence_bleu 이건 NLTK 라이브러리 함수인데 ref, gen 텍스트 간의 BLEU점수 계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a47ae3f-cf82-443d-9ae4-59ac4964cfe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [01:46<00:00,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3711369394618558\n",
      "Average BLEU Score: 0.9790509127219046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터셋에 대한 BLEU 점수 계산\n",
    "model.eval() # 모델을 평가모드로 설정\n",
    "total_eval_loss = 0 # 총 손실을 초기화\n",
    "bleu_scores = [] # bleu_scores를 넣을 리스트 초기화\n",
    "\n",
    "for batch in tqdm(validation_loader):\n",
    "    inputs, masks = [x.to(device) for x in batch]\n",
    "\n",
    "    with torch.no_grad(): # PyTorch 컨텍스트 매니저 / 이 블록에서 수행하는 연산은 그레디언트 추적 x / 이유는 메모리 절약, 계산효율성 증가, 모델 가중치 업데이트 방지\n",
    "        # forward 메서드를 사용하여 손실 계산\n",
    "        outputs = model(input_ids=inputs, attention_mask=masks, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # 모델의 출력 생성 (생성된 텍스트의 품질 평가용)\n",
    "        generated_outputs = model.generate(input_ids=inputs, max_length=100) # 모델을 활용해 새로운 텍스트 생성 / 길이는 100\n",
    "\n",
    "    # BLEU 점수 계산\n",
    "    for i in range(inputs.size(0)): # inputs.size(0)는 텐서의 첫번째 차원의 크기 / 텐서의 첫번째 차원의 크기 = 배치 크기\n",
    "        # decode는 encode한걸 다시 텍스트로 돌리기\n",
    "        ref_text = tokenizer.decode(inputs[i], skip_special_tokens=True) # inputs[i] : 현재 샘플의 입력 데이터, skip_special_tokens : 특수 토큰들 제외\n",
    "        gen_text = tokenizer.decode(generated_outputs[i], skip_special_tokens=True)\n",
    "        bleu_score = calculate_bleu([ref_text], gen_text)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "# 평균 손실 및 BLEU 점수 출력\n",
    "avg_val_loss = total_eval_loss / len(validation_loader)\n",
    "avg_bleu_score = np.mean(bleu_scores)\n",
    "print(f\"Validation Loss: {avg_val_loss}\")\n",
    "print(f\"Average BLEU Score: {avg_bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f929ea3-091f-4339-9374-de24f543fb6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 41/374 [00:12<01:45,  3.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m inputs, masks \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m      9\u001b[0m     total_eval_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:201\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# total_eval_loss = 0\n",
    "# for batch in tqdm(validation_loader):  # 검증 데이터 로더\n",
    "#     inputs, masks = [x.to(device) for x in batch]\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids=inputs, attention_mask=masks, labels=inputs)\n",
    "#         loss = outputs.loss\n",
    "#         total_eval_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce67e828-3a80-491c-9ade-bb43a80aef99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3711369394618558\n"
     ]
    }
   ],
   "source": [
    "# # 평균 검증 손실 계산\n",
    "# avg_val_loss = total_eval_loss / len(validation_loader)\n",
    "# print(f\"Validation Loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "065ab19b-cf80-49e5-b21e-6ca13a70b942",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def generate_response(sentence, model, tokenizer, device):\n",
    "#     # 입력 문장 토큰화\n",
    "#     input_ids = tokenizer.encode(sentence + tokenizer.eos_token, return_tensors='pt')\n",
    "#     input_ids = input_ids.to(device)\n",
    "\n",
    "#     # 모델이 응답 생성\n",
    "#     with torch.no_grad():\n",
    "#         output_ids = model.generate(input_ids, max_length=512, num_return_sequences=1)\n",
    "    \n",
    "#     # 생성된 토큰을 문자열로 변환\n",
    "#     full_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "#     # 입력 문장 이후의 부분만 반환\n",
    "#     # 가정: 응답은 '입력 문장 응답' 형식으로 구성됨\n",
    "#     response_start = full_response.find(sentence) + len(sentence)\n",
    "#     if response_start != -1:\n",
    "#         return full_response[response_start:].strip()\n",
    "#     else:\n",
    "#         return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fa6e480-1d25-46cb-9d25-3c40f61e005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 그냥 배달 수수료 있나요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 얼마예요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "만삼천원입니다\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 배달 시키면 배달 비 있어요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네, 배달비는 없어요.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1인분도 되요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 얼마 이상 배달비가 없어요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "만오천원 이상 배달비가 없어요\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 그 밑으로는요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 그 밑으로는요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 통순살흑마늘간장치킨 배달하면 배달비 얼마에요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000원이요\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 수고하세요\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네\n"
     ]
    }
   ],
   "source": [
    "# 예시 문장으로 응답 생성\n",
    "while True:\n",
    "    sentence = input()\n",
    "    response = generate_response(sentence, model, tokenizer, device)\n",
    "    print(response)\n",
    "    if sentence == '수고하세요':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88d6101e-d8ea-4a4d-98bd-d818f26e366b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/Project/SJ/saved_model/tokenizer_config.json',\n",
       " '/home/ubuntu/Project/SJ/saved_model/special_tokens_map.json',\n",
       " '/home/ubuntu/Project/SJ/saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 저장\n",
    "model_path = \"/home/ubuntu/Project/SJ/saved_model\"\n",
    "# save_pretrained(model_path) : PyTorch에서 huggingface의 transformers 라이브러리를 사용할 때 모델을 저장하는 방법\n",
    "# 모델의 가중치와 설정을 지정된 경로에 저장함\n",
    "model.save_pretrained(model_path)  \n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f3b14d-d85b-4c1d-bf81-451b5f02c63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sj",
   "language": "python",
   "name": "sj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
