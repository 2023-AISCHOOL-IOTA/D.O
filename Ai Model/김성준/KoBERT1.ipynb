{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"1JDeNu0In7f23eLj16-KkKTSBNYgFJ4XW","authorship_tag":"ABX9TyN0s6QcTkJLmQztgRRRLM+s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"phKc3-qElCb3","executionInfo":{"status":"ok","timestamp":1700217412188,"user_tz":-540,"elapsed":5139,"user":{"displayName":"정소이","userId":"03515220791152430617"}},"outputId":"2e1604ba-7532-4c4f-b0c0-37e134491048"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kobert-transformers in /usr/local/lib/python3.10/dist-packages (0.5.1)\n","Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from kobert-transformers) (2.1.0+cu118)\n","Requirement already satisfied: transformers<5,>=3 in /usr/local/lib/python3.10/dist-packages (from kobert-transformers) (4.35.2)\n","Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from kobert-transformers) (0.1.99)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (2.1.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->kobert-transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->kobert-transformers) (1.3.0)\n"]}],"source":["!pip install kobert-transformers"]},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bt9JpPg3m8YQ","executionInfo":{"status":"ok","timestamp":1700217417354,"user_tz":-540,"elapsed":5169,"user":{"displayName":"정소이","userId":"03515220791152430617"}},"outputId":"cc6e5132-0f79-4b79-88f3-8b030603c4bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"]}]},{"cell_type":"code","source":["!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lzj9F6wNnDUK","executionInfo":{"status":"ok","timestamp":1700217423379,"user_tz":-540,"elapsed":6028,"user":{"displayName":"정소이","userId":"03515220791152430617"}},"outputId":"c386be7f-6ad7-438c-c370-48e450898be8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-kfzotmmj/kobert-tokenizer_8c6ed3f96e8547b5939ee8379beb3017\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-kfzotmmj/kobert-tokenizer_8c6ed3f96e8547b5939ee8379beb3017\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AdamW\n","from kobert_transformers import get_kobert_model, get_tokenizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import pandas as pd\n","import numpy as np\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from transformers import BertForSequenceClassification\n","from sklearn.preprocessing import LabelEncoder"],"metadata":{"id":"MH2rYtjJlqDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_excel(\"/content/drive/MyDrive/젯봇/Data4.xlsx\", engine=\"openpyxl\")\n","le = LabelEncoder()\n","data['label_idx'] = le.fit_transform(data['label_idx'])\n","num_classes = data['label_idx'].nunique()"],"metadata":{"id":"d3Za2jSEl60U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1', last_hidden_states=True)\n","model = BertForSequenceClassification.from_pretrained('skt/kobert-base-v1', num_labels=num_classes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HdUPHNYzlyDH","executionInfo":{"status":"ok","timestamp":1700217430019,"user_tz":-540,"elapsed":1698,"user":{"displayName":"정소이","userId":"03515220791152430617"}},"outputId":"222f9d3e-ebdf-4362-db3a-45ac2a1edf4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n","The class this function is called from is 'KoBERTTokenizer'.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def convert_labels_to_numeric(label, num_classes):\n","    # 예: 3 -> [0, 0, 0, 1, 0, 0]\n","    labels = [0] * num_classes\n","    labels[label] = 1\n","    return labels\n"],"metadata":{"id":"AFOSrRwXppKJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class KoBERTDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len, num_classes):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.num_classes = num_classes\n","\n","    def __getitem__(self, index):\n","        text = self.data.SENTENCE[index]\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            truncation=True\n","        )\n","        label_str = self.data.label_idx[index]\n","        labels = convert_labels_to_numeric(label_str, self.num_classes)\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'targets': torch.tensor(labels, dtype=torch.float)  # 데이터 형식에 따라 수정\n","        }\n","\n","    def __len__(self):\n","        return self.len\n"],"metadata":{"id":"c3AeXzdwowqz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 128\n","BATCH_SIZE = 32\n","\n","train_data, val_data = train_test_split(data, test_size=0.3)\n","\n","train_data = train_data.reset_index(drop=True)\n","val_data = val_data.reset_index(drop=True)\n","\n","train_dataset = KoBERTDataset(train_data, tokenizer, MAX_LEN, num_classes)\n","val_dataset = KoBERTDataset(val_data, tokenizer, MAX_LEN, num_classes)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","class_weights = [0] * num_classes\n","label_counts = train_data['label_idx'].value_counts()\n","for label, count in label_counts.items():\n","  class_weights[label] = np.sum(label_counts) / count"],"metadata":{"id":"tB8zaUz3ozE4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","epochs = 3\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","total_steps = len(train_loader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(class_weights).to(device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-_VH6ATo-RU","executionInfo":{"status":"ok","timestamp":1700217433183,"user_tz":-540,"elapsed":2474,"user":{"displayName":"정소이","userId":"03515220791152430617"}},"outputId":"7eea7a5b-cbfe-45ee-937d-dd4026250cd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","for epoch in range(epochs):\n","    model.train()\n","    for batch in tqdm(train_loader, desc='Evaluating'):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        targets = batch['targets'].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        loss = loss_function(outputs.logits, targets)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCEhrjaKpA_T","executionInfo":{"status":"ok","timestamp":1700218096897,"user_tz":-540,"elapsed":649006,"user":{"displayName":"정소이","userId":"03515220791152430617"}},"outputId":"0c0c9a94-c60f-4290-bb01-d93af8302cc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 344/344 [03:37<00:00,  1.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3, Loss: 1.0718321800231934\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 344/344 [03:35<00:00,  1.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/3, Loss: 1.080198049545288\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 344/344 [03:35<00:00,  1.59it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/3, Loss: 0.9551646709442139\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","def evaluate_model(model, val_loader):\n","    model.eval()\n","    predictions, actuals = [], []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader, desc='Evaluating'):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            targets = batch['targets'].to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            preds = (torch.sigmoid(outputs.logits) > 0.5).cpu().detach().numpy()\n","            predictions.extend(preds)\n","            actuals.extend(targets.cpu().detach().numpy())\n","\n","    predictions = np.array(predictions)\n","    actuals = np.array(actuals)\n","\n","    accuracy = accuracy_score(actuals.ravel(), predictions.ravel())\n","    precision = precision_score(actuals.ravel(), predictions.ravel(), average='micro')\n","    recall = recall_score(actuals.ravel(), predictions.ravel(), average='micro')\n","    f1 = f1_score(actuals.ravel(), predictions.ravel(), average='micro')\n","\n","    return accuracy, precision, recall, f1\n","\n","\n","accuracy, precision, recall, f1 = evaluate_model(model, val_loader)\n","print(f\"Accuracy: {accuracy}\")\n","print(f\"Precision: {precision}\")\n","print(f\"Recall: {recall}\")\n","print(f\"F1 Score: {f1}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjhkGq4LpQN0","executionInfo":{"status":"ok","timestamp":1700218256187,"user_tz":-540,"elapsed":105791,"user":{"displayName":"정소이","userId":"03515220791152430617"}},"outputId":"1d58332e-152d-4330-a90d-da8f2b068af1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 148/148 [00:35<00:00,  4.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 0.7248967837510814\n","Precision: 0.7248967837510814\n","Recall: 0.7248967837510814\n","F1 Score: 0.7248967837510814\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"XdLYMUM-uTdD"},"execution_count":null,"outputs":[]}]}