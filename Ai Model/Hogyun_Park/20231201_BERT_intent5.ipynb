{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b84304-9d9d-4de3-b046-72d169aab036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.10/site-packages (0.1.99)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c778a362-e8f9-4f59-b4d8-5a45d62bfff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'KoBERT' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SKTBrain/KoBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d964b1-75c9-48a2-9d3b-72ddb5e5447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kobert_tokenizer\n",
      "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-8fy63cco/kobert-tokenizer_60fb8b3cfdd348a2a239be06482bb252\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-8fy63cco/kobert-tokenizer_60fb8b3cfdd348a2a239be06482bb252\n",
      "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1548f4-da3c-47aa-b55f-35154782d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454410a2-4d3e-4d2f-8929-22819f7621c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, AdamW, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba11aab3-251b-484f-b3f1-ba5c60174bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# csv 파일을 읽어오기\n",
    "data = pd.read_csv(\"restaurant_data2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad9a2f0f-9f3c-46df-be1f-37d35f7a8a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_only = data[[\"발화문\", \"인텐트\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7339d5ef-0111-4e07-a954-e17280c20337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# 토크나이저 초기화\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "# 데이터 임베딩\n",
    "embedding = tokenizer(data_only['발화문'].tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6a9326a-bdd3-481e-ac15-50cf7d1b5b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93411/260234296.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_only['인텐트'] = le.fit_transform(data_only['인텐트'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data_only['인텐트'] = le.fit_transform(data_only['인텐트'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca404919-7119-450d-bdb7-48201c72e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = embedding['input_ids']\n",
    "attention_masks = embedding['attention_mask']\n",
    "labels = data_only['인텐트'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69b2cad8-d4b9-4291-bdcf-c0467659c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MenuDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = [torch.tensor(ids, dtype=torch.long) for ids in input_ids]\n",
    "        self.attention_masks = [torch.tensor(mask, dtype=torch.long) for mask in attention_masks]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f347932-5a07-4db6-bfa7-555e2ab1d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "dataset = MenuDataset(input_ids=input_ids, attention_masks=attention_masks, labels=labels)\n",
    "\n",
    "# 데이터셋을 학습용과 검증용으로 분리\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cec05e88-210d-4881-aa54-39019d4d7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parmeter\n",
    "epochs= 5\n",
    "batch_size=32\n",
    "lr = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5320cac-0fe4-4799-8ec1-cd3885024f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_data_loader = DataLoader(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "284b54d8-b006-4d0e-be41-f56d114c19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch의 nn.Module 클래스를 상속받아 BertClassifier 클래스를 정의합니다.\n",
    "# 이 클래스는 BERT 모델을 이용한 분류 모델을 구현한 것입니다.\n",
    "class BertClassifier(nn.Module):\n",
    "    # 초기화 메소드에서는 분류할 라벨의 개수(num_labels)와 드롭아웃 비율(dropout_rate)를 인자로 받습니다.\n",
    "    # 'bert-base-uncased'라는 이름의 사전 훈련된 모델을 사용하여 BERT 모델을 초기화하고,\n",
    "    # 드롭아웃 레이어와 선형 레이어를 추가합니다.\n",
    "    def __init__(self, num_labels, dropout_rate=0.3):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(768, num_labels)\n",
    "\n",
    "    # forward 메소드는 입력 데이터(input_ids, attention_mask)를 받아 BERT 모델을 통과시키고,\n",
    "    # 그 결과를 드롭아웃 레이어와 선형 레이어를 통과시켜 최종 결과를 반환합니다.\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs['pooler_output']\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        return linear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28162bec-ae5e-427f-ad90-3d5e740e97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7260/7260 [11:25<00:00, 10.59it/s]\n",
      "Validating: 100%|██████████| 1815/1815 [00:59<00:00, 30.48it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6446 Accuracy: 0.7885 F1-score: 0.7786 Precision: 0.7833 Recall: 0.7885\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7260/7260 [11:25<00:00, 10.59it/s]\n",
      "Validating: 100%|██████████| 1815/1815 [00:59<00:00, 30.53it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6198 Accuracy: 0.7950 F1-score: 0.7856 Precision: 0.7912 Recall: 0.7950\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7260/7260 [11:25<00:00, 10.59it/s]\n",
      "Validating: 100%|██████████| 1815/1815 [00:59<00:00, 30.58it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6169 Accuracy: 0.8001 F1-score: 0.7905 Precision: 0.7952 Recall: 0.8001\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7260/7260 [11:25<00:00, 10.59it/s]\n",
      "Validating: 100%|██████████| 1815/1815 [00:59<00:00, 30.54it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6187 Accuracy: 0.8002 F1-score: 0.7920 Precision: 0.7948 Recall: 0.8002\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7260/7260 [11:25<00:00, 10.60it/s]\n",
      "Validating: 100%|██████████| 1815/1815 [00:59<00:00, 30.56it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6293 Accuracy: 0.8024 F1-score: 0.7969 Precision: 0.7986 Recall: 0.8024\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "num_labels = len(np.unique(data_only['인텐트']))\n",
    "model = BertClassifier(num_labels)\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "\n",
    "# 옵티마이저와 손실 함수 설정\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 학습 및 검증\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    for batch in tqdm(train_data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 검증 데이터 평가\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_predictions = []\n",
    "    val_truths = []\n",
    "    \n",
    "    for batch in tqdm(val_data_loader, desc=\"Validating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "    \n",
    "        val_losses.append(val_loss.item())\n",
    "        val_predictions.extend(torch.argmax(outputs, dim=1).cpu().detach().numpy().tolist())\n",
    "        val_truths.extend(labels.cpu().detach().numpy().tolist())\n",
    "    \n",
    "    val_loss = sum(val_losses) / len(val_losses)\n",
    "    val_acc = accuracy_score(val_truths, val_predictions)\n",
    "    val_f1 = f1_score(val_truths, val_predictions, average='weighted')\n",
    "    val_precision = precision_score(val_truths, val_predictions, average='weighted')\n",
    "    val_recall = recall_score(val_truths, val_predictions, average='weighted')\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss:.4f} Accuracy: {val_acc:.4f} F1-score: {val_f1:.4f} Precision: {val_precision:.4f} Recall: {val_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa8d4ecd-170a-450e-b981-ac032a23e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(text, model, tokenizer):\n",
    "    # 텍스트를 토크나이즈하고 BERT 입력 형식에 맞게 변환\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    \n",
    "    # 각 텐서를 GPU로 이동\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "    \n",
    "    # 모델의 예측 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 가장 높은 확률을 가진 클래스의 인덱스를 가져옴\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "    \n",
    "    # 예측된 인덱스를 의도로 변환\n",
    "    # 이 부분은 실제 의도와 인덱스를 매핑하는 방법에 따라 다르게 작성해야 합니다.\n",
    "    intent = le.inverse_transform([predicted.item()])[0]\n",
    "    \n",
    "    return intent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64186df8-f0a0-4742-8594-5b6d1a556071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 싸이 버거 얼마인가요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제품_가격_질문\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "text = input()\n",
    "predicted_intent = predict_intent(text, model, tokenizer)\n",
    "print(predicted_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69fc8049-1ed7-4d5f-a477-7c4825d828ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 state_dict 저장\n",
    "# torch.save(model.state_dict(), \"/home/ubuntu/Project/HG/saved_model3/Bert_model3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bc48614-fdcf-4f9e-95f4-12d5566f5930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "# LabelEncoder 저장\n",
    "# with open(\"/home/ubuntu/Project/HG/saved_model3/label_encoder.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f5c655f-79c2-4908-b91e-f57af470a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model_path = \"/home/ubuntu/Project/HG/saved_model_BERT\"  # 모델을 저장할 경로를 지정해주세요.\n",
    "model.bert.save_pretrained(model_path)\n",
    "\n",
    "# 토크나이저 저장\n",
    "tokenizer_path = \"/home/ubuntu/Project/HG/saved_model_BERT\"  # 토크나이저를 저장할 경로를 지정해주세요.\n",
    "tokenizer.save_pretrained(tokenizer_path)\n",
    "\n",
    "# 레이블 인코더 저장\n",
    "import pickle\n",
    "label_encoder_file = \"/home/ubuntu/Project/HG/saved_model_BERT/label_encoder.pkl\"  # 레이블 인코더를 저장할 경로를 지정해주세요.\n",
    "pickle.dump(le, open(label_encoder_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d27157-2acb-475d-9f96-f425150a7fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7730f-dd80-44a9-b9a4-9a6acd222018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e41330-8f48-44e5-8676-09dd4f6e0697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4c87c-e061-4bb6-b73e-cbe7b0a84e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842c3d0-d773-4685-8da6-5fd303166298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349111e9-01c3-44a0-ad32-26cb55ffdf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223c126-724e-4bba-929d-f760335efaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97a616-89cd-4742-b49b-dd22a59f1bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg",
   "language": "python",
   "name": "hg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
