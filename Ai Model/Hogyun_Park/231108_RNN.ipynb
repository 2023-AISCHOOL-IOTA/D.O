{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkGwyjkpdU9w",
        "outputId": "1829256c-1066-4ecb-c1d8-21b3f9942a7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9oGd8sgdSZS",
        "outputId": "fb2e4dfd-10f1-411f-e666-e6794536931b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/데이터전처리\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/데이터전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2d6zpkzdjoK",
        "outputId": "8ea015eb-c5e3-45e0-abae-bc6285e36b39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbQ25quyZ1Am"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import pprint # 보기 쉽게 깔끔한 형태로 출력\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "from torch import nn # 신경망(neural network)생성\n",
        "import torch.nn.functional as F # 활성화함수, 손실함수를 대체하여 코드를 간결하게 만듬\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "-KlGx8miZ1Ap",
        "outputId": "93857b68-1714-4404-c897-2b5ab93ebd69"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3aa71d1e-f9ba-4813-8729-d281ecbc26c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>네</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>네 접시 색깔은 다른데 가격은 똑같아요</td>\n",
              "      <td>1997</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3aa71d1e-f9ba-4813-8729-d281ecbc26c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3aa71d1e-f9ba-4813-8729-d281ecbc26c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3aa71d1e-f9ba-4813-8729-d281ecbc26c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-351aa3de-c286-4c56-8455-b2f63353bc4d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-351aa3de-c286-4c56-8455-b2f63353bc4d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-351aa3de-c286-4c56-8455-b2f63353bc4d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                sentence  label_idx\n",
              "0                      네          0\n",
              "1  네 접시 색깔은 다른데 가격은 똑같아요       1997"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(15726, 2)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_excel(\"./data/data.xlsx\", engine=\"openpyxl\")#.iloc[:100]\n",
        "# data[\"label_idx\"] = [random.choice([0,1,2,3,4]) for _ in range(100)]\n",
        "display(data.head(2))\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErzDbluxZ1Aq"
      },
      "outputs": [],
      "source": [
        "all_indices = data.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ts_fa0BZ1Ar"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(all_indices)) # data의 80% 훈련용\n",
        "val_size = len(all_indices) - train_size # data의 20% 검증용\n",
        "train_idx, val_idx = all_indices[:int(0.8 * len(all_indices))], all_indices[int(0.8 * len(all_indices)):]\n",
        "\n",
        "num_classes = max(data[\"label_idx\"].tolist())+1 # data 개수 == 인덱스 최대값 + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzgDZY-tZ1Ar",
        "outputId": "9de76da5-5446-4756-ec20-b16fb416c127"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(RangeIndex(start=0, stop=12580, step=1),\n",
              " RangeIndex(start=12580, stop=15726, step=1))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_idx, val_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpuLfNDCZ1Ar"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup # 학습 스케줄러를 생성\n",
        "from tqdm.auto import tqdm # 프로그램 실행 진행사항 확인\n",
        "\n",
        "# transformers 라이브러리에서 BERT 토크나이저를 임포트합니다.\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# 다국어 지원 BERT 모델을 사용하여 토크나이저 객체를 생성합니다.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JKDjCLWxgL_"
      },
      "source": [
        "- BERT 입력형태\n",
        "  - [CLS] 토큰 + 첫번째 문장 + [SEP] 토큰 + 두번째 문장\n",
        "    - 문장의 시작과 끝, 문장 간의 구분을 명확하게 인식 가능\n",
        "  - [CLS] 토큰\n",
        "    - \"Classification token\", 입력 문장의 시작 부분에 추가.\n",
        "    - 이 토큰의 임베딩은 문장 분류 작업에 사용, 문장 전체의 문맥을 요약하는 역할\n",
        "  - [SEP] 토큰\n",
        "    - \"Separator token\", 두 문장을 구분하는 데 사용\n",
        "  - 임베딩\n",
        "    - 텍스트 데이터를 컴퓨터가 이해하고 처리할 수 있는 수치형 벡터로 변환하는 과정 혹은 변환된 벡터\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR1M0LnQZ1As"
      },
      "outputs": [],
      "source": [
        "# gpu사용할 수 있는지 확인(사용O: cuda, 사용X: cpu)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.encodings = [\n",
        "            # - tokenizer(train_data[’sentence’].tolist()[0]) : batch 형태로 입력을 줌\n",
        "            # BERT에 필요한 입력 형태로 변환, 문장을 최대 길이에 맞게 패딩하고 결과값을 딕셔너리로 출력\n",
        "            # padding: 문장 길이 동일하게 만들기 위해 패딩 추가, truncation: 최대길이 초과하는 경우 잘라냄\n",
        "            tokenizer.encode_plus(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512) for sentence in df[\"sentence\"].tolist()\n",
        "        ]\n",
        "        self.labels = df[\"label_idx\"].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # to_return = {\n",
        "        #     \"sentence_encoded\":self.encodings[idx],\n",
        "        #     \"label\":self.labels[idx]\n",
        "        # }\n",
        "\n",
        "        # 인코딩된 문장, 레이블 쌍을 반환\n",
        "        return (self.encodings[idx], self.labels[idx])\n",
        "        # return to_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U4PwQWhZ1At"
      },
      "outputs": [],
      "source": [
        "train_data = data.iloc[train_idx]\n",
        "val_data = data.iloc[val_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfLh8sJHZ1At"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextClassificationDataset(train_data, tokenizer)\n",
        "val_dataset = TextClassificationDataset(val_data, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDDMGdK4Z1Au",
        "outputId": "c2da4843-c11d-4053-ef9a-3878e4d8ce43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'input_ids': tensor([[  0, 752,   2]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])},\n",
              " 0)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q64Ys18qZ1Au"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "\n",
        "# DataLoader : PyTorch에서 데이터 로드하는데 사용하는 클래스\n",
        "# 데이터셋을 batch_size만큼 분할하여 로드\n",
        "# shuffle=True: epoch마다 데이터셋을 무작위로 섞음(과적합 방지)\n",
        "# shuffle=False: 데이터셋을 섞지않음(검증 데이터는 순서가 결과에 영향을 미치지 않기때문)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHOObPSxZ1Au",
        "outputId": "708372b5-2ea6-4389-997c-a316cdd1c3f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 사전학습된 모델 불러옴\n",
        "# klue/roberta-samll: KLUE(Korean Language Understanding Evaluation) 벤티마크를 위해 학습된 Roberta 모델의 작은 버전\n",
        "model = AutoModel.from_pretrained(\"klue/roberta-small\")\n",
        "\n",
        "# 사용자 정의 모델 정의\n",
        "class CustomModel(nn.Module):\n",
        "    # hidden_size: hidden layer의 차원, 모델이 생성하는 벡터(임베딩)의 길이\n",
        "    # 클수록 더 많은 정보를 담을 수 있지만, 과적합 위험이 높음 + 계산 복잡도와 메모리 사용량 증가\n",
        "    def __init__(self, bert_model, num_classes, hidden_size):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.model = bert_model\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # 선형 변환을 수행하는 레이어\n",
        "        # BERT 모델의 출력을 입력받아 클래스의 수만큼 술력을 생성\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    # 모델 순전파\n",
        "    # BERT 모델을 통해 입력을 전달, 출력을 선형 레이어를 통해 전달하여 최종 출력을 생성\n",
        "    def forward(self, _input):\n",
        "        # pooler_output: 문장의 전체적인 의미를 압축적으로 담음\n",
        "        # [batch_size, hidden_size]형태의 텐서\n",
        "        output = self.model(**_input)[\"pooler_output\"]\n",
        "        output = self.fc(output[0])\n",
        "        return output\n",
        "\n",
        "\n",
        "custom_model = CustomModel(model, num_classes, 768)\n",
        "# AdamW: Adam optimizer의 변형, 가중치 감소(weight decay)를 지원\n",
        "# custom_model의 파라미터를 최적화\n",
        "optimizer = AdamW(custom_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsxOWU_lZ1Au",
        "outputId": "13a2e8df-89ff-4bc4-958b-3904fa9df3ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-44-d9d816a47877>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
            "<ipython-input-44-d9d816a47877>:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  loss = loss_f(F.softmax(outputs), _label)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "----------\n",
            "tensor([22.3561, -1.8212, -1.5098,  ..., -1.7176, -1.4976, -1.5368],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-44-d9d816a47877>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
            "<ipython-input-44-d9d816a47877>:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  all_predictions.append( torch.argmax(F.softmax(outputs)).item() )\n",
            "<ipython-input-44-d9d816a47877>:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  all_labels.append( torch.argmax(F.softmax(_label)).item() )\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2463445645263827\n",
            "0.09738180949142904\n",
            "Epoch 2/3\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-44-d9d816a47877>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
            "<ipython-input-44-d9d816a47877>:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  loss = loss_f(F.softmax(outputs), _label)\n"
          ]
        }
      ],
      "source": [
        "loss_f = nn.CrossEntropyLoss()\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    print('-' * 10)\n",
        "    # 모델을 학습 모드로 설정합니다.\n",
        "    model.train()\n",
        "    # 입력 데이터와 레이블 전처리\n",
        "    for i, (_input, _label) in enumerate(train_loader):\n",
        "        # 각 토큰의 인코딩을 포함하는 딕셔너리\n",
        "        _input = {k:v.squeeze(0) for k, v in _input.items()}\n",
        "        # 각 레이블을 원핫 인코딩으로 변환하고 텐서로 변환\n",
        "        _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
        "\n",
        "        outputs = custom_model(_input)\n",
        "        loss = loss_f(F.softmax(outputs), _label)\n",
        "        loss.backward() # 역전파를 수행합니다.\n",
        "        optimizer.step() # optimizer를 사용하여 파라미터 업데이터\n",
        "        optimizer.zero_grad() # 그라디언트 초기화\n",
        "\n",
        "    print(outputs)\n",
        "\n",
        "    # 모델을 평가 모드로 설정합니다.\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, (_input, _label) in enumerate(val_loader):\n",
        "        _input = {k:v.squeeze(0) for k, v in _input.items()}\n",
        "        _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
        "\n",
        "        outputs = custom_model(_input)\n",
        "        #loss = loss_f(F.softmax(outputs), _label)\n",
        "\n",
        "        # 모델의 예측과 실제 레이블을 저장\n",
        "        # 예측은 소프트맥스 함수로 확률 변환하고 가장 높은 확률을 가진 클래스를 선택\n",
        "        all_predictions.append( torch.argmax(F.softmax(outputs)).item() )\n",
        "        all_labels.append( torch.argmax(F.softmax(_label)).item() )\n",
        "\n",
        "\n",
        "    # 정확도와 가중치 평균 F! score 계산하여 출력\n",
        "    print(accuracy_score(all_labels, all_predictions))\n",
        "    print(f1_score(all_labels, all_predictions, average=\"weighted\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6GIGXmpZ1Av",
        "outputId": "79a8c7a0-74c3-411b-9dda-ba9e6c19672d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/3637145079.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  _label = torch.tensor(F.one_hot(_label, num_classes=num_classes), dtype=torch.float).squeeze()\n",
            "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/3637145079.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  all_predictions.append( torch.argmax(F.softmax(outputs)).item() )\n",
            "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/3637145079.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  all_labels.append( torch.argmax(F.softmax(_label)).item() )\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_q832DnZ1Av"
      },
      "outputs": [],
      "source": [
        "# 학습하기 이전 inference 결과\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiWRwVDYZ1Av"
      },
      "outputs": [],
      "source": [
        "# 학습하기 이전 inference 결과\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf49DvGPZ1Av"
      },
      "outputs": [],
      "source": [
        "_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAskSn1-Z1Av",
        "outputId": "49d4e22f-ff06-4e7d-e9a6-e71a95c8e118"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/3142704713.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  F.softmax(outputs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([8.3459e-08, 7.8275e-08, 1.0000e+00, 1.9766e-07, 1.6954e-07],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.softmax(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "077tTYREZ1Av",
        "outputId": "3895dc41-ca12-41c2-e08b-8be39dcf0629"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3g/y74d9tn12ls8tx3g7nmyjc3h0000gn/T/ipykernel_52307/3085399197.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  torch.argmax(F.softmax(outputs)).item()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmax(F.softmax(outputs)).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKVEQpbtZ1Aw",
        "outputId": "f8264ded-7ee2-4527-fee3-97ac172b315f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-3.6564, -3.7143, 11.9799, -2.7231, -2.9375], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWqJ93DNZ1Aw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
