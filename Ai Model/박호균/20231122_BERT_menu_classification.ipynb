{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc37688-e50e-4278-98a7-69f962079bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (0.1.99)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0cbdfff1-cf56-4a3c-9cff-05f4d66ae21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, AdamW, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bcad0635-8b26-493c-a6a1-106eed205b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 엑셀 파일을 읽어오기\n",
    "data = pd.read_excel(\"data.xlsx\", engine=\"openpyxl\") # openpyxl엔진을 쓰겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "49caf263-28b0-4b1e-b850-c13c4a136ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_only = data[[\"SENTENCE\", \"지식베이스\"]]\n",
    "# '지식베이스' 칼럼에서 결측치가 있는 행을 제거\n",
    "data_only = data_only.dropna(subset=['지식베이스'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5c6df8d2-d2f0-4b9f-a5e3-80e24336a58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>지식베이스</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>짬뽕류는 어떤 게 있나요? 잘 나가는 짬뽕 있나요?</td>\n",
       "      <td>짬뽕/메뉴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>특해물 짬뽕도 있고 전복 새우 짬뽕도 있고 해물 종류도 새우 홍합 전복 없는 게 없습니다</td>\n",
       "      <td>특해물 짬뽕/메뉴, 전복 새우 짬뽕/메뉴, 새우/해물/재료, 홍합/해물/재료, 전복...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>전복 들어가는 거는 특해물 짬뽕 시켜야 돼요?</td>\n",
       "      <td>전복/해물/재료, 특해물 짬뽕/메뉴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>전복 짬뽕 시키면 전복이 들어가죠</td>\n",
       "      <td>전복 짬뽕/메뉴, 전복/해물/재료</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>전복 들어가고 여러 가지 또 딴 것도 들어가죠?</td>\n",
       "      <td>전복/해물/재료</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            SENTENCE  \\\n",
       "2                       짬뽕류는 어떤 게 있나요? 잘 나가는 짬뽕 있나요?   \n",
       "3  특해물 짬뽕도 있고 전복 새우 짬뽕도 있고 해물 종류도 새우 홍합 전복 없는 게 없습니다   \n",
       "4                          전복 들어가는 거는 특해물 짬뽕 시켜야 돼요?   \n",
       "5                                 전복 짬뽕 시키면 전복이 들어가죠   \n",
       "6                         전복 들어가고 여러 가지 또 딴 것도 들어가죠?   \n",
       "\n",
       "                                               지식베이스  \n",
       "2                                              짬뽕/메뉴  \n",
       "3  특해물 짬뽕/메뉴, 전복 새우 짬뽕/메뉴, 새우/해물/재료, 홍합/해물/재료, 전복...  \n",
       "4                                전복/해물/재료, 특해물 짬뽕/메뉴  \n",
       "5                                 전복 짬뽕/메뉴, 전복/해물/재료  \n",
       "6                                           전복/해물/재료  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_only.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2c1a8b8d-1842-4fd2-b90e-e765f0b4cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \",\"를 하나의 token인 \"|\"으로 바꿔서 label을 표현\n",
    "data_only['지식베이스'] = data_only['지식베이스'].apply(lambda x: \"|\".join([s.strip() for s in x.split(\",\")]))\n",
    "data_only['메뉴정보'] = data_only['지식베이스'].apply(lambda x: 1 if '메뉴' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a5ace422-bdca-4280-afd8-7c38e4f10a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>지식베이스</th>\n",
       "      <th>메뉴정보</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>짬뽕류는 어떤 게 있나요? 잘 나가는 짬뽕 있나요?</td>\n",
       "      <td>짬뽕/메뉴</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>특해물 짬뽕도 있고 전복 새우 짬뽕도 있고 해물 종류도 새우 홍합 전복 없는 게 없습니다</td>\n",
       "      <td>특해물 짬뽕/메뉴|전복 새우 짬뽕/메뉴|새우/해물/재료|홍합/해물/재료|전복/해물/재료</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>전복 들어가는 거는 특해물 짬뽕 시켜야 돼요?</td>\n",
       "      <td>전복/해물/재료|특해물 짬뽕/메뉴</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>전복 짬뽕 시키면 전복이 들어가죠</td>\n",
       "      <td>전복 짬뽕/메뉴|전복/해물/재료</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>전복 들어가고 여러 가지 또 딴 것도 들어가죠?</td>\n",
       "      <td>전복/해물/재료</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            SENTENCE  \\\n",
       "2                       짬뽕류는 어떤 게 있나요? 잘 나가는 짬뽕 있나요?   \n",
       "3  특해물 짬뽕도 있고 전복 새우 짬뽕도 있고 해물 종류도 새우 홍합 전복 없는 게 없습니다   \n",
       "4                          전복 들어가는 거는 특해물 짬뽕 시켜야 돼요?   \n",
       "5                                 전복 짬뽕 시키면 전복이 들어가죠   \n",
       "6                         전복 들어가고 여러 가지 또 딴 것도 들어가죠?   \n",
       "\n",
       "                                              지식베이스  메뉴정보  \n",
       "2                                             짬뽕/메뉴     1  \n",
       "3  특해물 짬뽕/메뉴|전복 새우 짬뽕/메뉴|새우/해물/재료|홍합/해물/재료|전복/해물/재료     1  \n",
       "4                                전복/해물/재료|특해물 짬뽕/메뉴     1  \n",
       "5                                 전복 짬뽕/메뉴|전복/해물/재료     1  \n",
       "6                                          전복/해물/재료     0  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_only.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "edeb889f-15eb-4725-be30-7a9d725e736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# 토크나이저 초기화\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# 데이터 인코딩\n",
    "encodings = tokenizer(data_only['SENTENCE'].tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e87c78a8-9d95-4e2b-9906-1046b1c30411",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings['input_ids']\n",
    "attention_masks = encodings['attention_mask']\n",
    "labels = data_only['메뉴정보'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "01ee205d-9afc-415a-9faa-179f7e3dc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MenuDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = [torch.tensor(ids, dtype=torch.long) for ids in input_ids]\n",
    "        self.attention_masks = [torch.tensor(mask, dtype=torch.long) for mask in attention_masks]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "34d8c278-4cb2-45a0-a319-e8e566f0579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "dataset = MenuDataset(input_ids=input_ids, attention_masks=attention_masks, labels=labels)\n",
    "\n",
    "# 데이터셋을 학습용과 검증용으로 분리\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b3d160c5-fae1-42cc-bb7b-abbe5e491314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parmeter\n",
    "epochs= 5\n",
    "batch_size=32\n",
    "lr = 1e-5\n",
    "max_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cbaa7d6d-b21b-49fa-8a96-c80d6515f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_data_loader = DataLoader(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "68baa8e2-d76b-4998-b0ab-a7c30bac1fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AdamW, BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# BertClassifier 클래스 정의\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs['pooler_output']\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output).view(-1)  # 수정된 부분\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97289017-c1ff-4128-bef5-d556a6dcf911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 118/223 [04:02<03:34,  2.04s/it]"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = BertClassifier()\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 옵티마이저와 손실 함수 설정\n",
    "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\n",
    "criterion = nn.BCELoss().to(device)\n",
    "\n",
    "# 학습 및 검증\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    for batch in tqdm(train_data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 검증 데이터 평가\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_predictions = []\n",
    "    val_truths = []\n",
    "    \n",
    "    for batch in tqdm(val_data_loader, desc=\"Validating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            val_loss = criterion(outputs, labels.float())\n",
    "    \n",
    "        val_losses.append(val_loss.item())\n",
    "        val_predictions.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "        val_truths.extend(labels.cpu().detach().numpy().tolist())\n",
    "    \n",
    "    val_loss = sum(val_losses) / len(val_losses)\n",
    "    val_acc = accuracy_score(val_truths, [1 if pred >= 0.5 else 0 for pred in val_predictions])\n",
    "    val_f1 = f1_score(val_truths, [1 if pred >= 0.5 else 0 for pred in val_predictions])\n",
    "    val_precision = precision_score(val_truths, [1 if pred >= 0.5 else 0 for pred in val_predictions])\n",
    "    val_recall = recall_score(val_truths, [1 if pred >= 0.5 else 0 for pred in val_predictions])\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss:.4f} Accuracy: {val_acc:.4f} F1-score: {val_f1:.4f} Precision: {val_precision:.4f} Recall: {val_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be6ba4f-2b89-4155-ad07-409caad35d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sentence):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    tokenized_sentence = tokenizer.encode(sentence, add_special_tokens=True)  # 문장을 토큰화\n",
    "    input_ids = torch.tensor([tokenized_sentence]).to(device)  # 토큰화된 문장을 텐서로 변환\n",
    "    attention_mask = (input_ids != 0).float().to(device)  # attention mask 설정\n",
    "    with torch.no_grad():  # 기울기 계산 비활성화\n",
    "        proba = model(input_ids=input_ids, attention_mask=attention_mask).squeeze()  # 모델의 예측 확률값 계산\n",
    "    return 'Yes' if proba > 0.5 else 'No'  # 확률값에 따라 'Yes' 또는 'No' 반환\n",
    "\n",
    "sentence = \"짬뽕류는 어떤 게 있나요? 잘 나가는 짬뽕 있나요?\"\n",
    "print(predict(model, sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac0879-7915-45ee-8fe6-a9cd5f0590e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa5c7d-b189-420d-8f5b-4c307014068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "from transformers import BertPreTrainedModel\n",
    "from torch.optim import AdamW\n",
    "\n",
    "class BertForNER(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + (outputs.last_hidden_state,)  # Add hidden states and attention if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c602983-2f03-4970-9b02-f345c003dffb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_labels(sentence, knowledge_base, tokenizer, max_length):\n",
    "    if pd.isnull(sentence) or str(sentence).lower() == 'nan':  # sentence가 결측값인 경우 또는 'nan'인 경우\n",
    "        sentence = ''  # 빈 문자열로 설정\n",
    "    else:\n",
    "        sentence = str(sentence)  # 문자열로 변환\n",
    "    if pd.isnull(knowledge_base):  # knowledge_base가 결측값인 경우\n",
    "        knowledge_base = ''  # 빈 문자열로 설정\n",
    "    else:\n",
    "        knowledge_base = str(knowledge_base)  # 문자열로 변환\n",
    "\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    labels = [2] * len(tokens)  # 'O' -> 2\n",
    "    menus = knowledge_base.split('|')\n",
    "    for menu in menus:\n",
    "        if '/메뉴' in menu:\n",
    "            menu_name = menu.split('/메뉴')[0]\n",
    "            menu_tokens = tokenizer.tokenize(menu_name)\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i:i+len(menu_tokens)] == menu_tokens:\n",
    "                    labels[i] = 0  # 'B' -> 0\n",
    "                    for j in range(i+1, i+len(menu_tokens)):\n",
    "                        labels[j] = 1  # 'I' -> 1\n",
    "    \n",
    "    # Apply padding\n",
    "    if len(labels) < max_length:\n",
    "        labels += [2] * (max_length - len(labels))  # 'O' -> 2\n",
    "    else:\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248370a5-bdea-43d0-8f6b-f9d636f1e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MenuDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        sentence = row['SENTENCE']\n",
    "        knowledge_base = row['지식베이스']\n",
    "\n",
    "        # 결측값 또는 'nan'을 빈 문자열로 대체\n",
    "        if pd.isnull(sentence) or str(sentence).lower() == 'nan':\n",
    "            sentence = ''\n",
    "        else:\n",
    "            sentence = str(sentence)\n",
    "\n",
    "        if pd.isnull(knowledge_base) or str(knowledge_base).lower() == 'nan':\n",
    "            knowledge_base = ''\n",
    "        else:\n",
    "            knowledge_base = str(knowledge_base)\n",
    "\n",
    "        labels = extract_labels(sentence, knowledge_base, self.tokenizer, self.max_length)\n",
    "        encoding = self.tokenizer.encode_plus(sentence, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        items = {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "        items['labels'] = torch.tensor(labels, dtype=torch.long)  # 수정된 부분\n",
    "        return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7660f35-03f8-4611-a6be-0d63524a3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 가정: train_data와 val_data가 리스트 형태로 제공되었다.\n",
    "train_data = pd.DataFrame(train_data, columns=['SENTENCE', '지식베이스'])\n",
    "val_data = pd.DataFrame(val_data, columns=['SENTENCE', '지식베이스'])\n",
    "\n",
    "# 데이터 로더 생성\n",
    "MAX_LENGTH = 128  # 또는 원하는 값을 설정하세요.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "train_dataset = MenuDataset(train_data, tokenizer, MAX_LENGTH)\n",
    "val_dataset = MenuDataset(val_data, tokenizer, MAX_LENGTH)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4232d-8161-4263-ba63-53d71e6dbf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "num_labels = 10  # 추출할 라벨의 수를 설정합니다.\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')  # BERT의 설정을 불러옵니다.\n",
    "config.num_labels = num_labels  # 라벨의 수를 설정합니다.\n",
    "\n",
    "model2 = BertForNER(config, num_labels).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)  # 옵티마이저 설정\n",
    "\n",
    "epochs=5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    model2.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[0]\n",
    "        preds = torch.argmax(logits, dim=2)\n",
    "        val_preds.extend(preds.view(-1).cpu().numpy())\n",
    "        val_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    print(f'Train loss  : {avg_train_loss:.4f}')\n",
    "    print(f'Val accuracy: {val_acc:.4f}')\n",
    "    print(f'Val precision: {val_precision:.4f}')\n",
    "    print(f'Val recall   : {val_recall:.4f}')\n",
    "    print(f'Val f1-score : {val_f1:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92696883-d46a-4052-8260-a9747df1dc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab280e-2055-4853-b133-e86826067484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55878c-c5c5-4748-99f1-5204e2fbd7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
