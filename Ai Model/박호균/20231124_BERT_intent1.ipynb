{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b84304-9d9d-4de3-b046-72d169aab036",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/miniforge3/envs/tf212_py310/lib/python3.10/site-packages (0.1.99)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "454410a2-4d3e-4d2f-8929-22819f7621c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sklearn.model_selection 모듈의 train_test_split 함수는 데이터를 학습 데이터와 테스트 데이터로 분할하는데 사용됩니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# transformers의 BertTokenizer, AdamW, BertModel은 BERT 모델(자연어처리를 위한 트랜스포머 기반 모델)을 사용하는데 필요한 클래스입니다.\n",
    "from transformers import BertTokenizer, AdamW, BertModel\n",
    "\n",
    "# torch.utils.data의 Dataset, DataLoader는 파이토치(PyTorch)에서 데이터를 로드하고 전처리하는데 사용되는 클래스입니다.\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# nn과 torch는 파이토치 라이브러리로, 딥러닝 모델을 구현하는데 사용됩니다. nn은 신경망을 구성하는데 필요한 다양한 모듈과 손실 함수를 제공하며, torch는 텐서 등의 기본 연산을 제공합니다.\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# sklearn.metrics의 accuracy_score, f1_score, precision_score, recall_score는 모델의 성능을 평가하는데 사용되는 메트릭 함수입니다.\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba11aab3-251b-484f-b3f1-ba5c60174bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일을 읽어오기\n",
    "data = pd.read_csv(\"restaurant_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad9a2f0f-9f3c-46df-be1f-37d35f7a8a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_only = data[[\"발화문\", \"인텐트\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7339d5ef-0111-4e07-a954-e17280c20337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers 라이브러리에서 BertTokenizerFast를 임포트합니다.\n",
    "# BertTokenizerFast는 BERT 토크나이저의 빠른 버전입니다.\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# 'bert-base-uncased'라는 이름의 사전 훈련된 모델을 사용하여 토크나이저를 초기화합니다.\n",
    "# 'bert-base-uncased'는 가장 기본적인 BERT 모델입니다.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# data_only['발화문'].tolist()를 사용하여 토크나이저를 통해 데이터를 인코딩합니다.\n",
    "# truncation=True는 문장이 모델의 최대 문장 길이를 초과할 경우 잘라내는 역할을 합니다.\n",
    "# padding=True는 모든 문장을 모델의 최대 문장 길이에 맞게 패딩하는 역할을 합니다.\n",
    "encodings = tokenizer(data_only['발화문'].tolist(), truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6a9326a-bdd3-481e-ac15-50cf7d1b5b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30607/260234296.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_only['인텐트'] = le.fit_transform(data_only['인텐트'])\n"
     ]
    }
   ],
   "source": [
    "# sklearn.preprocessing 모듈의 LabelEncoder를 임포트합니다.\n",
    "# LabelEncoder는 범주형 변수를 정수로 인코딩하는데 사용되는 클래스입니다.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# LabelEncoder 인스턴스를 생성합니다.\n",
    "le = LabelEncoder()\n",
    "\n",
    "# '인텐트'라는 열에 대해 fit_transform 메소드를 적용하여 범주형 라벨을 정수로 인코딩합니다.\n",
    "# fit_transform 메소드는 fit 메소드(라벨에 대한 인코딩 규칙을 학습)와 transform 메소드(해당 규칙을 데이터에 적용)를 순차적으로 수행합니다.\n",
    "data_only['인텐트'] = le.fit_transform(data_only['인텐트'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca404919-7119-450d-bdb7-48201c72e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩된 데이터에서 'input_ids'를 가져와 input_ids 변수에 저장합니다.\n",
    "# 'input_ids'는 토큰화된 각 토큰을 인덱스로 변환한 결과를 담고 있습니다.\n",
    "input_ids = encodings['input_ids']\n",
    "\n",
    "# 인코딩된 데이터에서 'attention_mask'를 가져와 attention_masks 변수에 저장합니다.\n",
    "# 'attention_mask'는 패딩된 부분과 실제 토큰화된 부분을 구분하기 위한 마스크입니다.\n",
    "attention_masks = encodings['attention_mask']\n",
    "\n",
    "# '인텐트' 열의 값을 numpy 배열로 변환하여 labels 변수에 저장합니다.\n",
    "# 이는 모델 학습에 사용될 타겟 값(라벨)입니다.\n",
    "labels = data_only['인텐트'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69b2cad8-d4b9-4291-bdcf-c0467659c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch의 Dataset 클래스를 상속받아 MenuDataset 클래스를 정의합니다.\n",
    "# 이 클래스는 BERT 모델에 입력될 데이터를 관리하고, 각각의 데이터를 반환하는 기능을 제공합니다.\n",
    "class MenuDataset(Dataset):\n",
    "    # 초기화 메소드에서는 input_ids, attention_masks, labels를 인자로 받아 클래스의 멤버 변수로 저장합니다.\n",
    "    # input_ids와 attention_masks는 리스트 형태의 데이터를 PyTorch의 tensor 데이터로 변환하여 저장합니다.\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = [torch.tensor(ids, dtype=torch.long) for ids in input_ids]\n",
    "        self.attention_masks = [torch.tensor(mask, dtype=torch.long) for mask in attention_masks]\n",
    "        self.labels = labels\n",
    "\n",
    "    # __len__ 메소드는 데이터셋의 총 데이터 개수를 반환합니다.\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    # __getitem__ 메소드는 주어진 인덱스에 해당하는 데이터를 반환합니다.\n",
    "    # 여기서는 input_ids, attention_mask, labels를 포함한 딕셔너리 형태로 반환하고 있습니다.\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f347932-5a07-4db6-bfa7-555e2ab1d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "dataset = MenuDataset(input_ids=input_ids, attention_masks=attention_masks, labels=labels)\n",
    "\n",
    "# 데이터셋을 학습용과 검증용으로 분리\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cec05e88-210d-4881-aa54-39019d4d7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parmeter\n",
    "\n",
    "# 전체 데이터셋에 대한 학습 반복 회수\n",
    "epochs= 5\n",
    "# 한 번의 학습 단계에서 사용할 데이터의 개수\n",
    "batch_size=32 \n",
    "# learning_rate(학습률)\n",
    "# 경사 하강법에서 한 스텝의 크기를 결정\n",
    "lr = 5e-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5320cac-0fe4-4799-8ec1-cd3885024f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_data_loader = DataLoader(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "284b54d8-b006-4d0e-be41-f56d114c19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch의 nn.Module 클래스를 상속받아 BertClassifier 클래스를 정의합니다.\n",
    "# 이 클래스는 BERT 모델을 이용한 분류 모델을 구현한 것입니다.\n",
    "class BertClassifier(nn.Module):\n",
    "    # 초기화 메소드에서는 분류할 라벨의 개수(num_labels)와 드롭아웃 비율(dropout_rate)를 인자로 받습니다.\n",
    "    # 'bert-base-uncased'라는 이름의 사전 훈련된 모델을 사용하여 BERT 모델을 초기화하고,\n",
    "    # 드롭아웃 레이어와 선형 레이어를 추가합니다.\n",
    "    def __init__(self, num_labels, dropout_rate=0.3):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(768, num_labels)\n",
    "\n",
    "    # forward 메소드는 입력 데이터(input_ids, attention_mask)를 받아 BERT 모델을 통과시키고,\n",
    "    # 그 결과를 드롭아웃 레이어와 선형 레이어를 통과시켜 최종 결과를 반환합니다.\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs['pooler_output']\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        return linear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28162bec-ae5e-427f-ad90-3d5e740e97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7260/7260 [28:26<00:00,  4.25it/s]\n",
      "Validating: 100%|██████████| 1815/1815 [02:22<00:00, 12.73it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0636 Accuracy: 0.4731 F1-score: 0.4493 Precision: 0.4631 Recall: 0.4731\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7260/7260 [28:26<00:00,  4.25it/s]\n",
      "Validating: 100%|██████████| 1815/1815 [02:22<00:00, 12.73it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.9098 Accuracy: 0.5012 F1-score: 0.4862 Precision: 0.5067 Recall: 0.5012\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7260/7260 [28:27<00:00,  4.25it/s]\n",
      "Validating: 100%|██████████| 1815/1815 [02:22<00:00, 12.74it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.8356 Accuracy: 0.5174 F1-score: 0.5046 Precision: 0.5255 Recall: 0.5174\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7260/7260 [28:27<00:00,  4.25it/s]\n",
      "Validating: 100%|██████████| 1815/1815 [02:22<00:00, 12.73it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.8245 Accuracy: 0.5251 F1-score: 0.5135 Precision: 0.5337 Recall: 0.5251\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7260/7260 [28:26<00:00,  4.25it/s]\n",
      "Validating: 100%|██████████| 1815/1815 [02:22<00:00, 12.76it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.8018 Accuracy: 0.5301 F1-score: 0.5193 Precision: 0.5433 Recall: 0.5301\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7260/7260 [28:27<00:00,  4.25it/s]\n",
      "Validating: 100%|██████████| 1815/1815 [02:22<00:00, 12.73it/s]\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.8011 Accuracy: 0.5313 F1-score: 0.5238 Precision: 0.5478 Recall: 0.5313\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 5872/7260 [23:00<05:26,  4.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 27\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 검증 데이터 평가\u001b[39;00m\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/optimization.py:467\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m    466\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[0;32m--> 467\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    468\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    470\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# '인텐트' 열의 라벨 개수를 구하고, 이를 사용하여 BertClassifier 모델을 생성합니다.\n",
    "num_labels = len(np.unique(data_only['인텐트']))\n",
    "model = BertClassifier(num_labels)\n",
    "\n",
    "# GPU를 사용하기 위해 디바이스를 설정하고, 모델을 해당 디바이스로 이동시킵니다.\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "\n",
    "# 옵티마이저와 손실 함수를 설정합니다. \n",
    "# 여기서는 AdamW 옵티마이저와 크로스 엔트로피 손실 함수를 사용합니다.\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# 주어진 에폭 수만큼 학습을 반복합니다.\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # 학습 모드로 설정합니다.\n",
    "    model.train()\n",
    "    \n",
    "    # 학습 데이터 로더에서 배치 단위로 데이터를 가져옵니다.\n",
    "    for batch in tqdm(train_data_loader, desc=\"Training\"):\n",
    "        # 배치에서 데이터를 가져와 GPU로 이동시킵니다.\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # 모델에 입력 데이터를 넣어 출력 결과를 얻습니다.\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # 손실 함수에 출력 결과와 타깃 라벨을 넣어 손실 값을 계산합니다.\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 경사를 초기화하고, 역전파를 수행하고, 가중치를 업데이트합니다.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 검증 모드로 설정합니다.\n",
    "    model.eval()\n",
    "    \n",
    "    # 검증 데이터에 대한 손실 값을 저장할 리스트와 예측 결과 및 실제 라벨을 저장할 리스트를 생성합니다.\n",
    "    val_losses = []\n",
    "    val_predictions = []\n",
    "    val_truths = []\n",
    "    \n",
    "    # 검증 데이터 로더에서 배치 단위로 데이터를 가져옵니다.\n",
    "    for batch in tqdm(val_data_loader, desc=\"Validating\"):\n",
    "        # 배치에서 데이터를 가져와 GPU로 이동시킵니다.\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "    \n",
    "        # 그래디언트 계산을 비활성화하고, 모델에 입력 데이터를 넣어 출력 결과를 얻습니다.\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "    \n",
    "        # 손실 값을, 예측 결과, 실제 라벨을 저장합니다.\n",
    "        val_losses.append(val_loss.item())\n",
    "        val_predictions.extend(torch.argmax(outputs, dim=1).cpu().detach().numpy().tolist())\n",
    "        val_truths.extend(labels.cpu().detach().numpy().tolist())\n",
    "    \n",
    "    # 평균 손실 값을, 정확도, F1 점수, 정밀도, 재현율을 계산합니다.\n",
    "    val_loss = sum(val_losses) / len(val_losses)\n",
    "    val_acc = accuracy_score(val_truths, val_predictions)\n",
    "    val_f1 = f1_score(val_truths, val_predictions, average='weighted')\n",
    "    val_precision = precision_score(val_truths, val_predictions, average='weighted')\n",
    "    val_recall = recall_score(val_truths, val_predictions, average='weighted')\n",
    "    \n",
    "    # 계산된 메트릭 값을 출력합니다.\n",
    "    print(f\"Validation Loss: {val_loss:.4f} Accuracy: {val_acc:.4f} F1-score: {val_f1:.4f} Precision: {val_precision:.4f} Recall: {val_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa8d4ecd-170a-450e-b981-ac032a23e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(text, model, tokenizer):\n",
    "    # 텍스트를 토크나이즈하고 BERT 입력 형식에 맞게 변환\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    \n",
    "    # 각 텐서를 GPU로 이동\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "    \n",
    "    # 모델의 예측 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 가장 높은 확률을 가진 클래스의 인덱스를 가져옴\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "    \n",
    "    # 예측된 인덱스를 의도로 변환\n",
    "    # 이 부분은 실제 의도와 인덱스를 매핑하는 방법에 따라 다르게 작성해야 합니다.\n",
    "    intent = le.inverse_transform([predicted.item()])[0]\n",
    "    \n",
    "    return intent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "64186df8-f0a0-4742-8594-5b6d1a556071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 배달료 따로 있나요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배송_비용_질문\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "text = input()\n",
    "predicted_intent = predict_intent(text, model, tokenizer)\n",
    "print(predicted_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "69fc8049-1ed7-4d5f-a477-7c4825d828ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 state_dict 저장\n",
    "torch.save(model.state_dict(), \"/home/ubuntu/Project/HG/saved_model1/Bert_model1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ac7db-26e8-421c-b706-405992c21979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7bc48614-fdcf-4f9e-95f4-12d5566f5930",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# LabelEncoder 저장\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ubuntu/Project/HG/saved_model1/label_encoder.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241m.\u001b[39mdump(le, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "# LabelEncoder 저장\n",
    "with open(\"/home/ubuntu/Project/HG/saved_model1/label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca03d96-1445-4dd6-ada0-ea218a900afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c655f-79c2-4908-b91e-f57af470a651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d27157-2acb-475d-9f96-f425150a7fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7730f-dd80-44a9-b9a4-9a6acd222018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e41330-8f48-44e5-8676-09dd4f6e0697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b4c87c-e061-4bb6-b73e-cbe7b0a84e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842c3d0-d773-4685-8da6-5fd303166298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349111e9-01c3-44a0-ad32-26cb55ffdf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223c126-724e-4bba-929d-f760335efaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97a616-89cd-4742-b49b-dd22a59f1bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
